2024-11-21 05:43:16,585 The args: Namespace(data_dir='./glue_data/CoLA', teacher_model='./models/bert-base-uncased', student_model='./models/TinyBERT_General_4L_312D', task_name='cola', output_dir='task_distill_output_try', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=True, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=10.0, warmup_proportion=0.1, no_cuda=False, seed=42, gradient_accumulation_steps=1, aug_train=False, eval_step=50, pred_distill=False, data_url='', temperature=1.0)
2024-11-21 05:43:16,585 device: cpu n_gpu: 0
2024-11-21 05:43:16,648 Writing example 0 of 8551
2024-11-21 05:43:16,648 *** Example ***
2024-11-21 05:43:16,648 guid: train-0
2024-11-21 05:43:16,648 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2024-11-21 05:43:16,648 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2024-11-21 05:43:16,648 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2024-11-21 05:43:16,648 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2024-11-21 05:43:16,648 label: 1
2024-11-21 05:43:16,648 label_id: 1
2024-11-21 05:43:17,421 Writing example 0 of 1043
2024-11-21 05:43:17,421 *** Example ***
2024-11-21 05:43:17,421 guid: dev-0
2024-11-21 05:43:17,421 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2024-11-21 05:43:17,421 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2024-11-21 05:43:17,421 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2024-11-21 05:43:17,421 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2024-11-21 05:43:17,421 label: 1
2024-11-21 05:43:17,421 label_id: 1
2024-11-21 05:43:17,515 Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "training": "",
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2024-11-21 05:43:19,431 Loading model ./models/bert-base-uncased\pytorch_model.bin
2024-11-21 05:43:19,958 loading model...
2024-11-21 05:43:20,121 done!
2024-11-21 05:43:20,121 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2024-11-21 05:43:20,121 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2024-11-21 05:43:20,137 Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2024-11-21 05:43:20,420 Loading model ./models/TinyBERT_General_4L_312D\pytorch_model.bin
2024-11-21 05:43:20,532 loading model...
2024-11-21 05:43:20,532 done!
2024-11-21 05:43:20,532 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2024-11-21 05:43:20,532 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias']
2024-11-21 05:43:20,550 ***** Running training *****
2024-11-21 05:43:20,550   Num examples = 8551
2024-11-21 05:43:20,564   Batch size = 32
2024-11-21 05:43:20,564   Num steps = 2670
2024-11-21 05:43:20,580 n: bert.embeddings.word_embeddings.weight
2024-11-21 05:43:20,580 n: bert.embeddings.position_embeddings.weight
2024-11-21 05:43:20,580 n: bert.embeddings.token_type_embeddings.weight
2024-11-21 05:43:20,580 n: bert.embeddings.LayerNorm.weight
2024-11-21 05:43:20,580 n: bert.embeddings.LayerNorm.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.attention.self.query.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.attention.self.query.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.attention.self.key.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.attention.self.key.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.attention.self.value.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.attention.self.value.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.attention.output.dense.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.attention.output.dense.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.intermediate.dense.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.intermediate.dense.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.output.dense.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.output.dense.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.output.LayerNorm.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.0.output.LayerNorm.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.attention.self.query.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.attention.self.query.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.attention.self.key.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.attention.self.key.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.attention.self.value.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.attention.self.value.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.attention.output.dense.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.attention.output.dense.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.intermediate.dense.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.intermediate.dense.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.output.dense.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.output.dense.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.output.LayerNorm.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.1.output.LayerNorm.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.attention.self.query.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.attention.self.query.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.attention.self.key.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.attention.self.key.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.attention.self.value.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.attention.self.value.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.attention.output.dense.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.attention.output.dense.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.intermediate.dense.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.intermediate.dense.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.output.dense.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.output.dense.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.output.LayerNorm.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.2.output.LayerNorm.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.attention.self.query.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.attention.self.query.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.attention.self.key.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.attention.self.key.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.attention.self.value.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.attention.self.value.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.attention.output.dense.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.attention.output.dense.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.intermediate.dense.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.intermediate.dense.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.output.dense.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.output.dense.bias
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.output.LayerNorm.weight
2024-11-21 05:43:20,580 n: bert.encoder.layer.3.output.LayerNorm.bias
2024-11-21 05:43:20,580 n: bert.pooler.dense.weight
2024-11-21 05:43:20,580 n: bert.pooler.dense.bias
2024-11-21 05:43:20,580 n: classifier.weight
2024-11-21 05:43:20,580 n: classifier.bias
2024-11-21 05:43:20,580 n: fit_dense.weight
2024-11-21 05:43:20,580 n: fit_dense.bias
2024-11-21 05:43:20,580 Total parameters: 14591258
2024-11-21 05:49:28,766 ***** Running evaluation *****
2024-11-21 05:49:28,782   Epoch = 0 iter 49 step
2024-11-21 05:49:28,782   Num examples = 1043
2024-11-21 05:49:28,798   Batch size = 32
2024-11-21 05:49:28,798 ***** Eval results *****
2024-11-21 05:49:28,798   att_loss = 0.4968893211715075
2024-11-21 05:49:28,798   cls_loss = 0.0
2024-11-21 05:49:28,798   global_step = 49
2024-11-21 05:49:28,798   loss = 1.7810000327168678
2024-11-21 05:49:28,798   rep_loss = 1.2841107127617817
2024-11-21 05:49:28,798 ***** Save model *****
2024-11-21 05:56:10,863 ***** Running evaluation *****
2024-11-21 05:56:10,863   Epoch = 0 iter 99 step
2024-11-21 05:56:10,863   Num examples = 1043
2024-11-21 05:56:10,863   Batch size = 32
2024-11-21 05:56:10,879 ***** Eval results *****
2024-11-21 05:56:10,879   att_loss = 0.4594799415631728
2024-11-21 05:56:10,879   cls_loss = 0.0
2024-11-21 05:56:10,879   global_step = 99
2024-11-21 05:56:10,879   loss = 1.5233127252020018
2024-11-21 05:56:10,879   rep_loss = 1.0638327827357283
2024-11-21 05:56:10,879 ***** Save model *****
2024-11-21 06:02:50,485 ***** Running evaluation *****
2024-11-21 06:02:50,485   Epoch = 0 iter 149 step
2024-11-21 06:02:50,485   Num examples = 1043
2024-11-21 06:02:50,485   Batch size = 32
2024-11-21 06:02:50,501 ***** Eval results *****
2024-11-21 06:02:50,501   att_loss = 0.4356956937969131
2024-11-21 06:02:50,501   cls_loss = 0.0
2024-11-21 06:02:50,501   global_step = 149
2024-11-21 06:02:50,501   loss = 1.3959508950278263
2024-11-21 06:02:50,501   rep_loss = 0.9602552004308509
2024-11-21 06:02:50,501 ***** Save model *****
2024-11-21 06:09:02,218 ***** Running evaluation *****
2024-11-21 06:09:02,234   Epoch = 0 iter 199 step
2024-11-21 06:09:02,234   Num examples = 1043
2024-11-21 06:09:02,234   Batch size = 32
2024-11-21 06:09:02,234 ***** Eval results *****
2024-11-21 06:09:02,234   att_loss = 0.4242985676880458
2024-11-21 06:09:02,234   cls_loss = 0.0
2024-11-21 06:09:02,234   global_step = 199
2024-11-21 06:09:02,234   loss = 1.3222903678165607
2024-11-21 06:09:02,234   rep_loss = 0.8979917977323484
2024-11-21 06:09:02,234 ***** Save model *****
2024-11-21 06:15:17,305 ***** Running evaluation *****
2024-11-21 06:15:17,305   Epoch = 0 iter 249 step
2024-11-21 06:15:17,305   Num examples = 1043
2024-11-21 06:15:17,305   Batch size = 32
2024-11-21 06:15:17,305 ***** Eval results *****
2024-11-21 06:15:17,305   att_loss = 0.41571395919026144
2024-11-21 06:15:17,305   cls_loss = 0.0
2024-11-21 06:15:17,305   global_step = 249
2024-11-21 06:15:17,305   loss = 1.2716423924189495
2024-11-21 06:15:17,305   rep_loss = 0.8559284313136793
2024-11-21 06:15:17,305 ***** Save model *****
2024-11-21 06:21:26,751 ***** Running evaluation *****
2024-11-21 06:21:26,751   Epoch = 1 iter 299 step
2024-11-21 06:21:26,751   Num examples = 1043
2024-11-21 06:21:26,751   Batch size = 32
2024-11-21 06:21:26,751 ***** Eval results *****
2024-11-21 06:21:26,751   att_loss = 0.3723100945353508
2024-11-21 06:21:26,751   cls_loss = 0.0
2024-11-21 06:21:26,751   global_step = 299
2024-11-21 06:21:26,751   loss = 1.0400353707373142
2024-11-21 06:21:26,751   rep_loss = 0.667725270614028
2024-11-21 06:21:26,751 ***** Save model *****
2024-11-21 06:27:36,518 ***** Running evaluation *****
2024-11-21 06:27:36,518   Epoch = 1 iter 349 step
2024-11-21 06:27:36,518   Num examples = 1043
2024-11-21 06:27:36,518   Batch size = 32
2024-11-21 06:27:36,518 ***** Eval results *****
2024-11-21 06:27:36,518   att_loss = 0.3764837144351587
2024-11-21 06:27:36,518   cls_loss = 0.0
2024-11-21 06:27:36,518   global_step = 349
2024-11-21 06:27:36,518   loss = 1.0397016827653094
2024-11-21 06:27:36,518   rep_loss = 0.6632179676032648
2024-11-21 06:27:36,518 ***** Save model *****
